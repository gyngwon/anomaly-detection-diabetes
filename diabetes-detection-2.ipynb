{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb380a2",
   "metadata": {},
   "source": [
    "# Diabetes Prediction and Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb1067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import tensorflow as tf\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from scipy import stats\n",
    "from pylab import rcParams\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9281a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596457bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.info()\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447c21d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=msno.bar(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82919725",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922e22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Outcome'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9815d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization to check if the target is balanced or imbalanced\n",
    "plt.figure(figsize=(4, 4))\n",
    "\n",
    "# Count the occurrences of each outcome\n",
    "outcome_counts = df['Outcome'].value_counts()\n",
    "\n",
    "# Create the bar plot\n",
    "outcome_counts.plot(kind='bar', color=['#6f8ea5', '#dfd9e1'])\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Distribution of Target Variable (Outcome)', fontsize=14)\n",
    "plt.xlabel('Outcome', fontsize=10)\n",
    "plt.ylabel('Frequency', fontsize=10)\n",
    "\n",
    "# Add value labels on top of the bars\n",
    "for index, value in enumerate(outcome_counts):\n",
    "    plt.text(index, value, str(value), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels if necessary\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbab3129",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = df.drop('Outcome', axis=1)\n",
    "y = df['Outcome']\n",
    "\n",
    "# Upsampling the normal data ratio to 95% for anomaly detection tasks\n",
    "ratio = 0.95\n",
    "curr_ratio = y.value_counts(normalize=True)[0]\n",
    "\n",
    "# Calculate the required sampling strategy for oversampling\n",
    "sampling_strategy_value = (ratio * (1 - curr_ratio)) / (curr_ratio * (1 - ratio))\n",
    "sampling_strategy = {0: int(y.value_counts()[0] * (1 + sampling_strategy_value))}\n",
    "\n",
    "# Create RandomOverSampler object\n",
    "ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "\n",
    "# Apply oversampling\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "y_resampled_series = pd.Series(y_resampled)\n",
    "\n",
    "# Convert X_resampled to a pandas DataFrame\n",
    "X_resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "\n",
    "# Add noise to the BMI and DiabetesPedigreeFunction columns\n",
    "noise_bmi = np.random.normal(loc=0, scale=0.01, size=X_resampled.shape[0])\n",
    "noise_dpf = np.random.normal(loc=0, scale=0.01, size=X_resampled.shape[0])\n",
    "\n",
    "X_resampled_df['BMI'] += noise_bmi\n",
    "X_resampled_df['DiabetesPedigreeFunction'] += noise_dpf\n",
    "\n",
    "# Print the results\n",
    "print(\"Data size after oversampling:\", X_resampled_df.shape)\n",
    "print(\"Class distribution after oversampling:\\n\", y_resampled_series.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aba662",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Outcome', axis=1)\n",
    "y = df['Outcome']\n",
    "\n",
    "# Set the target ratio for the majority class (normal data, class 0) to 95%\n",
    "target_ratio = 0.95\n",
    "current_ratio = y.value_counts(normalize=True)[0]\n",
    "\n",
    "# Calculate the sampling strategy for oversampling the minority class (class 1)\n",
    "sampling_strategy_value = (target_ratio * (1 - current_ratio)) / (current_ratio * (1 - target_ratio))\n",
    "sampling_strategy = {0: int(y.value_counts()[0] * (1 + sampling_strategy_value))}\n",
    "\n",
    "# Create a RandomOverSampler object with the calculated sampling strategy\n",
    "ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "\n",
    "# Apply RandomOverSampler to resample the dataset\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Convert the resampled target y_resampled to a pandas Series for easier manipulation\n",
    "y_resampled_series = pd.Series(y_resampled)\n",
    "\n",
    "# Convert X_resampled back to a pandas DataFrame with the original feature names\n",
    "X_resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "\n",
    "# Add some noise to the 'BMI' and 'DiabetesPedigreeFunction' columns\n",
    "noise_bmi = np.random.normal(loc=0, scale=0.01, size=X_resampled.shape[0])\n",
    "noise_dpf = np.random.normal(loc=0, scale=0.01, size=X_resampled.shape[0])\n",
    "\n",
    "X_resampled_df['BMI'] += noise_bmi\n",
    "X_resampled_df['DiabetesPedigreeFunction'] += noise_dpf\n",
    "\n",
    "# Print the resampled data shape and class distribution\n",
    "print(\"Resampled data shape:\", X_resampled_df.shape)\n",
    "print(\"Resampled class distribution:\\n\", y_resampled_series.value_counts())\n",
    "\n",
    "# Plot the resampled class distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x=y_resampled_series)\n",
    "plt.title('Resampled Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e211376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the visual style of the plots\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Plot the distribution of 'BMI' and 'DiabetesPedigreeFunction' to compare the original vs resampled datasets\n",
    "\n",
    "# Create a function to plot distributions\n",
    "def plot_distribution(original_data, resampled_data, feature_name):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Original data plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(original_data[feature_name], kde=True, bins=20, color='blue', alpha=0.6)\n",
    "    plt.title(f'Original {feature_name} Distribution', fontsize=14)\n",
    "    plt.xlabel(feature_name, fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "\n",
    "    # Resampled data plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(resampled_data[feature_name], kde=True, bins=20, color='orange', alpha=0.6)\n",
    "    plt.title(f'Resampled {feature_name} Distribution with Noise', fontsize=14)\n",
    "    plt.xlabel(feature_name, fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot distributions for 'BMI' and 'DiabetesPedigreeFunction'\n",
    "plot_distribution(X, X_resampled_df, 'BMI')\n",
    "plot_distribution(X, X_resampled_df, 'DiabetesPedigreeFunction')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889cff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization process\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_resampled_df)\n",
    "df_scaled = scaler.transform(X_resampled_df)\n",
    "\n",
    "# Convert to DataFrame and copy\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=X_resampled_df.columns)\n",
    "x_df_scaled = df_scaled.copy()\n",
    "\n",
    "# Add target column (check if indices match)\n",
    "df_scaled['target'] = y_resampled_series.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54605699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "correlation_matrix = df_scaled.corr()\n",
    "\n",
    "# Display the correlation with the target variable\n",
    "target_correlation = correlation_matrix['target'].sort_values(ascending=False)\n",
    "print(target_correlation)\n",
    "\n",
    "# Set the size of the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', square=True)\n",
    "\n",
    "# Title and labels\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27642ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum number of iterations for the model\n",
    "log_reg = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Train the logistic regression model\n",
    "log_reg.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Calculate feature importance using regression coefficients\n",
    "importance = log_reg.coef_[0]  # Get coefficients of the trained model\n",
    "feature_importance = pd.Series(importance, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = feature_importance.plot(kind='bar', color='skyblue')\n",
    "plt.title('Feature Importance using Logistic Regression', fontsize=16)\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.ylabel('Importance', fontsize=12)\n",
    "\n",
    "# Add coefficient values as annotations on the bars\n",
    "for i in ax.patches:\n",
    "    ax.annotate(f'{i.get_height():.2f}', \n",
    "                (i.get_x() + i.get_width() / 2., i.get_height()), \n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40edd087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the random forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)  # Set random_state for reproducibility\n",
    "\n",
    "# Train the random forest model\n",
    "rf_model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Calculate feature importance\n",
    "importance = rf_model.feature_importances_\n",
    "\n",
    "# Convert feature importance to a Series and sort\n",
    "feature_importance = pd.Series(importance, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = feature_importance.plot(kind='bar', color='lightgreen')\n",
    "plt.title('Feature Importance using Random Forest', fontsize=16)\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.ylabel('Importance', fontsize=12)\n",
    "\n",
    "# Add importance values as annotations on the bars\n",
    "for i in ax.patches:\n",
    "    ax.annotate(f'{i.get_height():.2f}', \n",
    "                (i.get_x() + i.get_width() / 2., i.get_height()), \n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ad6e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled_df, y_resampled_series, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c1c209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution in y_train\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "\n",
    "print(\"Class distribution in y_train:\")\n",
    "print(class_distribution)\n",
    "print(209/4478)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70bed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution in y_test\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "\n",
    "print(\"Class distribution in y_test:\")\n",
    "print(class_distribution)\n",
    "print(59/1113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bed13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Select only the Glucose and DiabetesPedigreeFunction features\n",
    "X_train_subset = X_train[['Glucose', 'DiabetesPedigreeFunction']]\n",
    "X_test_subset = X_test[['Glucose', 'DiabetesPedigreeFunction']]\n",
    "\n",
    "# Train Isolation Forest model\n",
    "iso_forest = IsolationForest(contamination=0.095, random_state=42)\n",
    "iso_forest.fit(X_train_subset)\n",
    "\n",
    "# Predict anomalies\n",
    "y_pred_test = iso_forest.predict(X_test_subset)\n",
    "y_true_binary = (y_test == 1)\n",
    "y_pred_binary = (y_pred_test == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f72a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_true_binary, y_pred_binary)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_true_binary, y_pred_binary)\n",
    "precision = precision_score(y_true_binary, y_pred_binary)\n",
    "recall = recall_score(y_true_binary, y_pred_binary)\n",
    "f1 = f1_score(y_true_binary, y_pred_binary)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa0c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Class SVM for Anomaly Detection\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Select features\n",
    "X_train_subset = X_train[['Glucose', 'DiabetesPedigreeFunction']]\n",
    "X_test_subset = X_test[['Glucose', 'DiabetesPedigreeFunction']]\n",
    "\n",
    "# Define One-Class SVM model\n",
    "oc_svm = OneClassSVM()\n",
    "\n",
    "# Set hyperparameter grid for random search\n",
    "param_dist = {\n",
    "    'nu': [0.01, 0.05, 0.1, 0.2, 0.5],  # Candidate values for nu\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  # Types of kernels\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1, 1, 10]  # Candidate values for gamma\n",
    "}\n",
    "\n",
    "# Define custom scorer for performance evaluation\n",
    "def custom_scorer(y_true, y_pred):\n",
    "    # In binary classification, set anomalies as the positive class (1)\n",
    "    y_true_binary = (y_true == 1)\n",
    "    y_pred_binary = (y_pred == -1)\n",
    "\n",
    "    return f1_score(y_true_binary, y_pred_binary)\n",
    "\n",
    "scorer = make_scorer(custom_scorer, greater_is_better=True)\n",
    "\n",
    "# Create random search object\n",
    "random_search = RandomizedSearchCV(oc_svm,\n",
    "                                   param_distributions=param_dist,\n",
    "                                   n_iter=50, scoring=scorer,\n",
    "                                   refit=True, cv=5, random_state=42)\n",
    "\n",
    "# Train random search model\n",
    "random_search.fit(X_train_subset, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best Parameters: \", random_search.best_params_)\n",
    "\n",
    "# Predict on test data with the best model\n",
    "best_oc_svm = random_search.best_estimator_\n",
    "y_pred_test = best_oc_svm.predict(X_test_subset)\n",
    "y_true_binary = (y_test == 1)\n",
    "y_pred_binary = (y_pred_test == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd8b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_true_binary, y_pred_binary)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_true_binary, y_pred_binary)\n",
    "precision = precision_score(y_true_binary, y_pred_binary)\n",
    "recall = recall_score(y_true_binary, y_pred_binary)\n",
    "f1 = f1_score(y_true_binary, y_pred_binary)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a5c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diabetes Prediction Using Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Select Glucose and DiabetesPedigreeFunction features\n",
    "X_train_RFC = X_train[['Glucose', 'DiabetesPedigreeFunction']]\n",
    "X_test_RFC = X_test[['Glucose', 'DiabetesPedigreeFunction']]\n",
    "\n",
    "# Define Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Set hyperparameter range for random search\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Create random search object\n",
    "random_search = RandomizedSearchCV(estimator=rf_model,\n",
    "                                   param_distributions=param_dist,\n",
    "                                   n_iter=100, scoring='accuracy',\n",
    "                                   cv=5, random_state=42,\n",
    "                                   n_jobs=-1, verbose=2)\n",
    "\n",
    "# Train random search model\n",
    "random_search.fit(X_train_RFC, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best Parameters: \", random_search.best_params_)\n",
    "\n",
    "# Predict on test data with the best model\n",
    "best_rf_model = random_search.best_estimator_\n",
    "y_pred_test_RFC = best_rf_model.predict(X_test_RFC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd285b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm_RFC = confusion_matrix(y_test, y_pred_test_RFC)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_RFC)\n",
    "\n",
    "# Calculate performance metrics for binary classification\n",
    "accuracy_RFC = accuracy_score(y_test, y_pred_test_RFC)\n",
    "precision_RFC = precision_score(y_test, y_pred_test_RFC, average='macro')\n",
    "recall_RFC = recall_score(y_test, y_pred_test_RFC, average='macro')\n",
    "f1_RFC = f1_score(y_test, y_pred_test_RFC, average='macro')\n",
    "\n",
    "print(\"Accuracy:\", accuracy_RFC)\n",
    "print(\"Precision:\", precision_RFC)\n",
    "print(\"Recall:\", recall_RFC)\n",
    "print(\"F1 Score:\", f1_RFC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa7f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_df_scaled, y_resampled_series, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b131e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "# Use only Glucose feature for training\n",
    "X_train = X_train.iloc[:, [1]]\n",
    "X_test = X_test.iloc[:, [1]]\n",
    "\n",
    "# Initialize Elliptic Envelope model\n",
    "elliptic_env = EllipticEnvelope(contamination=0.04)  # contamination represents the ratio of anomalies\n",
    "\n",
    "# Train the Elliptic Envelope model\n",
    "elliptic_env.fit(X_train)\n",
    "\n",
    "# Predict (1: normal, -1: anomaly)\n",
    "predictions = elliptic_env.predict(X_test)\n",
    "\n",
    "# Convert predictions to 0 (normal) and 1 (anomaly)\n",
    "predictions = np.where(predictions == 1, 0, 1)\n",
    "\n",
    "# Add predictions as a new column\n",
    "elliptic_X_test = X_test.copy()\n",
    "elliptic_X_test['anomaly'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14717c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual and predicted values\n",
    "actual = y_test  # Actual labels\n",
    "predicted = elliptic_X_test['anomaly']  # Predicted labels\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(actual, predicted)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(actual, predicted)\n",
    "precision = precision_score(actual, predicted)\n",
    "recall = recall_score(actual, predicted)\n",
    "f1 = f1_score(actual, predicted)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61145d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_df_scaled, y_resampled_series, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dab747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only normal data (0) for training\n",
    "X_train_normal = X_train[y_train == 0]\n",
    "\n",
    "# Output results\n",
    "print(\"Size of training normal data:\", X_train_normal.shape)\n",
    "print(\"Class distribution of training normal data:\\n\", y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5645cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set visualization libraries\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 8\n",
    "\n",
    "# Set RANDOM_SEED and LABELS\n",
    "RANDOM_SEED = 42\n",
    "LABELS = [\"Normal\", \"Fraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ccf64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model design\n",
    "input_dim = X_train.shape[1]  # Input dimension (number of features)\n",
    "encoding_dim = 15  # Dimension to compress to\n",
    "\n",
    "# Model structure\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "encoder = Dense(encoding_dim, activation=\"relu\",\n",
    "                activity_regularizer=regularizers.l1(1e-5))(input_layer)\n",
    "encoder = Dense(int(encoding_dim / 2), activation=\"relu\")(encoder)\n",
    "decoder = Dense(int(encoding_dim / 2), activation='relu')(encoder)\n",
    "decoder = Dense(input_dim, activation='linear')(decoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340efc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the autoencoder model\n",
    "nb_epoch = 500\n",
    "batch_size = 64\n",
    "\n",
    "autoencoder.compile(optimizer='adam',\n",
    "                    loss='mean_squared_error',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# Fit the model using only normal data for training\n",
    "history = autoencoder.fit(X_train_normal, X_train_normal,\n",
    "                          epochs=nb_epoch,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          validation_data=(X_test, X_test),  # Validate with both normal and abnormal data\n",
    "                          verbose=1).history\n",
    "\n",
    "# Recompile the model to ensure metrics are included\n",
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='adam')\n",
    "\n",
    "# Save the best model during training\n",
    "cp = ModelCheckpoint(filepath=\"/kaggle/working/autoencoder_classifier.keras\",\n",
    "                     save_best_only=True,\n",
    "                     verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee90ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'history' is your history dictionary containing 'loss' and 'val_loss'\n",
    "epochs = list(range(len(history['loss'])))\n",
    "\n",
    "# Create traces for training and validation loss\n",
    "train_loss = go.Scatter(x=epochs, y=history['loss'], mode='lines', name='Train Loss')\n",
    "val_loss = go.Scatter(x=epochs, y=history['val_loss'], mode='lines', name='Validation Loss')\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(\n",
    "    title='Model Loss',\n",
    "    xaxis=dict(title='Epoch'),\n",
    "    yaxis=dict(title='Loss'),\n",
    ")\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure(data=[train_loss, val_loss], layout=layout)\n",
    "\n",
    "# Show the plot\n",
    "pyo.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c681bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate reconstruction errors on the test data\n",
    "reconstructions = autoencoder.predict(X_test)\n",
    "reconstruction_errors = np.mean(np.square(X_test - reconstructions), axis=1)\n",
    "\n",
    "# Predict anomalies based on reconstruction errors\n",
    "threshold = 0.18  # Set threshold for identifying anomalies\n",
    "predicted_anomalies = (reconstruction_errors > threshold).astype(int)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, predicted_anomalies)\n",
    "accuracy = accuracy_score(y_test, predicted_anomalies)\n",
    "precision = precision_score(y_test, predicted_anomalies)\n",
    "recall = recall_score(y_test, predicted_anomalies)\n",
    "f1 = f1_score(y_test, predicted_anomalies)\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeaa63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for reconstruction errors\n",
    "error_df = pd.DataFrame({'reconstruction_error': reconstruction_errors,\n",
    "                         'true_class': y_test})\n",
    "\n",
    "groups = error_df.groupby('true_class')\n",
    "\n",
    "# Create traces for each class\n",
    "traces = []\n",
    "for name, group in groups:\n",
    "    trace = go.Scatter(\n",
    "        x=group.index,\n",
    "        y=group.reconstruction_error,\n",
    "        mode='markers',\n",
    "        marker=dict(size=3.5),\n",
    "        name=\"Fraud\" if name == 1 else \"Normal\"\n",
    "    )\n",
    "    traces.append(trace)\n",
    "\n",
    "# Add threshold line\n",
    "threshold_line = go.Scatter(\n",
    "    x=[error_df.index.min(), error_df.index.max()],\n",
    "    y=[threshold, threshold],\n",
    "    mode='lines',\n",
    "    line=dict(color='red', dash='dash'),\n",
    "    name='Threshold'\n",
    ")\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(\n",
    "    title=\"Reconstruction Error for Different Classes\",\n",
    "    xaxis=dict(title=\"Data Point Index\"),\n",
    "    yaxis=dict(title=\"Reconstruction Error\"),\n",
    "    yaxis_range=[0, 1.5],\n",
    ")\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure(data=traces + [threshold_line], layout=layout)\n",
    "\n",
    "# Show the plot\n",
    "pyo.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32e4eae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
